{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/weizheng/Documents/data_science_data/data_mining_assignment2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row_ID,Household_ID,Vehicle,Calendar_Year,Model_Year,Blind_Make,Blind_Model,Blind_Submodel,Cat1,Cat2,Cat3,Cat4,Cat5,Cat6,Cat7,Cat8,Cat9,Cat10,Cat11,Cat12,OrdCat,Var1,Var2,Var3,Var4,Var5,Var6,Var7,Var8,NVCat,NVVar1,NVVar2,NVVar3,NVVar4,Claim_Amount\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 train_set.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row_ID,Household_ID,Vehicle,Calendar_Year,Model_Year,Blind_Make,Blind_Model,Blind_Submodel,Cat1,Cat2,Cat3,Cat4,Cat5,Cat6,Cat7,Cat8,Cat9,Cat10,Cat11,Cat12,OrdCat,Var1,Var2,Var3,Var4,Var5,Var6,Var7,Var8,NVCat,NVVar1,NVVar2,NVVar3,NVVar4\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 test_set.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let pandas read csv and determine datatype\n",
    "df_train = pd.read_csv(\"train_set.csv\")\n",
    "df_test = pd.read_csv(\"test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Claim_Amount'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_train.columns) - set(df_test.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row_ID              int64\n",
       "Household_ID        int64\n",
       "Vehicle             int64\n",
       "Calendar_Year       int64\n",
       "Model_Year          int64\n",
       "Blind_Make         object\n",
       "Blind_Model        object\n",
       "Blind_Submodel     object\n",
       "Cat1               object\n",
       "Cat2               object\n",
       "Cat3               object\n",
       "Cat4               object\n",
       "Cat5               object\n",
       "Cat6               object\n",
       "Cat7               object\n",
       "Cat8               object\n",
       "Cat9               object\n",
       "Cat10              object\n",
       "Cat11              object\n",
       "Cat12              object\n",
       "OrdCat             object\n",
       "Var1              float64\n",
       "Var2              float64\n",
       "Var3              float64\n",
       "Var4              float64\n",
       "Var5              float64\n",
       "Var6              float64\n",
       "Var7              float64\n",
       "Var8              float64\n",
       "NVCat              object\n",
       "NVVar1            float64\n",
       "NVVar2            float64\n",
       "NVVar3            float64\n",
       "NVVar4            float64\n",
       "Claim_Amount      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. how many rows and columns does train_set have? How many rows and columns does test_set have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set\n",
      "row: 13184290\n",
      "column: 35\n",
      "\n",
      "test_set\n",
      "row: 4314865\n",
      "column: 34\n"
     ]
    }
   ],
   "source": [
    "print \"train_set\\nrow: {0}\\ncolumn: {1}\\n\\ntest_set\\nrow: {2}\\ncolumn: {3}\".format(*df_train.shape + df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use sqlite instead of pandas groupby because of memory limitation\n",
    "del df_test\n",
    "del df_train\n",
    "#load to sqlite and release memoryp\n",
    "con = sqlite3.connect(\"allclaim.db\")\n",
    "pd.io.sql.write_frame(df_train, \"train_set\",con)\n",
    "df_test.to_sql(\"test_set\", con)\n",
    "cursor = con.cursor()\n",
    "#create index \n",
    "cursor.execute(\"create index train_ind on train_set('Household_ID')\")\n",
    "cursor.execute(\"create index test_ind on test_set('Household_ID')\")\n",
    "#add new features to train set, only row with non_zero claim amount is a claim\n",
    "sql = \"create table train_grouped as select household_id, count(NULLIF(claim_amount,0)) as total_claim_count, sum(claim_amount) as total_claim_amount from train_set \" + \\\n",
    "      \"group by household_id;\"\n",
    "cursor.execute(sql)\n",
    "sql = \"create table train_with_new_features as \" + \\\n",
    "      \"select t.*, g.total_claim_count, g.total_claim_amount \" + \\\n",
    "      \"from train_grouped as g inner join train_set as t on g.household_id = t.household_id;\"\n",
    "cursor.execute(sql)\n",
    "sql = \"create index train_with_new_features_ind on train_with_new_features(household_id);\"\n",
    "cursor.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add new features to test set\n",
    "sql = \"create table test_with_new_features as \" + \\\n",
    "      \"select te.*, tr.total_claim_count, tr.total_claim_amount \" + \\\n",
    "      \"from test_set as te \" + \\\n",
    "      \"left outer join train_with_new_features as tr \" + \\\n",
    "      \"on te.household_id = tr.household_id;\"\n",
    "cursor.execute(sql)\n",
    "#set total_claim_count, total_claim_amount to 0  \n",
    "sql = \"Update test_with_new_features \" + \\\n",
    "      \"Set total_claim_count = 0, total_claim_amount = 0 \" + \\\n",
    "      \"Where total_claim_amount IS NULL;\"\n",
    "cursor.execute(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a copy of test_set\n",
    "sql = \"create table test_with_new_features as \" + \\\n",
    "      \"select * \" + \\\n",
    "      \"from test_set;\"\n",
    "cursor.execute(sql)\n",
    "#add feature columns\n",
    "sql = \"Alter table test_with_new_features Add column total_claim_count INTEGER ;\"\n",
    "cursor.execute(sql)\n",
    "sql = \"Alter table test_with_new_features Add column total_claim_amount REAL ;\"\n",
    "cursor.execute(sql)\n",
    "#update new features \n",
    "sql = \"UPDATE test_with_new_features \" + \\\n",
    "      \"SET \" + \\\n",
    "          \"total_claim_count = (SELECT total_claim_count FROM train_with_new_features WHERE household_id = test_with_new_features.household_id), \" + \\\n",
    "          \"total_claim_amount = (SELECT total_claim_amount FROM train_with_new_features WHERE household_id = test_with_new_features.household_id);\"\n",
    "cursor.execute(sql)\n",
    "sql = \"create index test_with_new_features_ind on test_with_new_features(household_id);\"\n",
    "cursor.execute(sql)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. In train_set how many past claims does household 4719940 have? Total claim amount?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Household 4719940 in train_set\n",
      "Total Number of Claims: 1.0\n",
      "Total Claim Amount: 562.4142\n"
     ]
    }
   ],
   "source": [
    "sql = \"select total_claim_count, total_claim_amount \" + \\\n",
    "      \"from train_grouped \" + \\\n",
    "      \"where household_id = 4719940;\"\n",
    "df_4 = pd.read_sql(sql,con)\n",
    "print \"Household 4719940 in train_set\\nTotal Number of Claims: {0}\\nTotal Claim Amount: {1}\".format(*list(df_4.ix[0,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In test_set how many past claims does household 3372413 have? Total claim amount?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Household 3372413 in test_set\n",
      "Total Number of Claims: 1.0\n",
      "Total Claim Amount: 1390.159\n"
     ]
    }
   ],
   "source": [
    "sql = \"select total_claim_count, total_claim_amount \" + \\\n",
    "      \"from test_with_new_features \" + \\\n",
    "      \"where household_id = 3372413;\"\n",
    "df_3 = pd.read_sql(sql,con)\n",
    "print \"Household 3372413 in test_set\\nTotal Number of Claims: {0}\\nTotal Claim Amount: {1}\".format(*list(df_3.ix[0,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. How many levels of the categorical variable blind_model exist in the test_set but not in the train_set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql = \"select distinct(Blind_Model) from train_with_new_features;\"\n",
    "train_lvls = pd.read_sql(sql, con);\n",
    "sql = \"select distinct(Blind_Model) from test_with_new_features;\"\n",
    "test_lvls = pd.read_sql(sql, con);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_lvls_set = set(test_lvls.Blind_Model.values)\n",
    "train_lvls_set = set(train_lvls.Blind_Model.values)\n",
    "lvl_only_in_tst = test_lvls_set - train_lvls_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'AF.2',\n",
       " u'AG.1',\n",
       " u'AI.14',\n",
       " u'AI.31',\n",
       " u'AI.41',\n",
       " u'AI.43',\n",
       " u'AI.44',\n",
       " u'AI.8',\n",
       " u'AI.9',\n",
       " u'AL.102',\n",
       " u'AL.12',\n",
       " u'AL.15',\n",
       " u'AL.2',\n",
       " u'AL.25',\n",
       " u'AL.26',\n",
       " u'AL.29',\n",
       " u'AL.30',\n",
       " u'AL.32',\n",
       " u'AL.35',\n",
       " u'AL.40',\n",
       " u'AL.48',\n",
       " u'AL.55',\n",
       " u'AL.57',\n",
       " u'AL.65',\n",
       " u'AL.78',\n",
       " u'AL.79',\n",
       " u'AL.86',\n",
       " u'AL.89',\n",
       " u'AN.10',\n",
       " u'AN.21',\n",
       " u'AN.4',\n",
       " u'AO.2',\n",
       " u'AP.2',\n",
       " u'AR.12',\n",
       " u'AR.8',\n",
       " u'AR.9',\n",
       " u'AU.10',\n",
       " u'AY.32',\n",
       " u'AY.33',\n",
       " u'AY.42',\n",
       " u'AY.50',\n",
       " u'AY.57',\n",
       " u'AY.68',\n",
       " u'AZ.20',\n",
       " u'AZ.27',\n",
       " u'BF.13',\n",
       " u'BF.14',\n",
       " u'BF.45',\n",
       " u'BG.23',\n",
       " u'BG.24',\n",
       " u'BH.21',\n",
       " u'BH.25',\n",
       " u'BO.13',\n",
       " u'BP.16',\n",
       " u'BP.4',\n",
       " u'BQ.8',\n",
       " u'BT.53',\n",
       " u'BT.68',\n",
       " u'BT.71',\n",
       " u'BT.75',\n",
       " u'BU.34',\n",
       " u'BU.6',\n",
       " u'BU.7',\n",
       " u'BW.189',\n",
       " u'BW.38',\n",
       " u'BW.39',\n",
       " u'BY.8',\n",
       " u'BY.9',\n",
       " u'BZ.1',\n",
       " u'BZ.18',\n",
       " u'BZ.19',\n",
       " u'BZ.3',\n",
       " u'BZ.31',\n",
       " u'D.14',\n",
       " u'D.4',\n",
       " u'E.10',\n",
       " u'E.18',\n",
       " u'E.21',\n",
       " u'I.26',\n",
       " u'I.27',\n",
       " u'I.37',\n",
       " u'I.38',\n",
       " u'J.11',\n",
       " u'J.12',\n",
       " u'J.8',\n",
       " u'K.41',\n",
       " u'K.43',\n",
       " u'K.5',\n",
       " u'L.22',\n",
       " u'L.39',\n",
       " u'L.40',\n",
       " u'L.47',\n",
       " u'L.50',\n",
       " u'L.58',\n",
       " u'M.5',\n",
       " u'M.7',\n",
       " u'N.13',\n",
       " u'N.14',\n",
       " u'N.15',\n",
       " u'N.16',\n",
       " u'O.21',\n",
       " u'O.22',\n",
       " u'O.7',\n",
       " u'O.8',\n",
       " u'P.12',\n",
       " u'P.14',\n",
       " u'P.17',\n",
       " u'P.26',\n",
       " u'P.28',\n",
       " u'P.4',\n",
       " u'P.5',\n",
       " u'Q.24',\n",
       " u'Q.28',\n",
       " u'Q.32',\n",
       " u'Q.33',\n",
       " u'R.2',\n",
       " u'R.3',\n",
       " u'R.33',\n",
       " u'R.44',\n",
       " u'T.2',\n",
       " u'X.20',\n",
       " u'X.30',\n",
       " u'Y.12',\n",
       " u'Y.25',\n",
       " u'Y.35',\n",
       " u'Y.37'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lvl_only_in_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lvl_only_in_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use Weight of Evidence to fix problematic, high-cardinality categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WOE\n",
      "X_45: 9.69782946805\n",
      "X_45_2: 11.5378133717\n"
     ]
    }
   ],
   "source": [
    "sql = \"select claim_amount from train_with_new_features;\"\n",
    "all_claims = pd.read_sql(sql, con).values\n",
    "sql = \"select claim_amount from train_with_new_features where blind_model = 'X.45';\"\n",
    "x_45 = pd.read_sql(sql, con).values\n",
    "sql = \"select claim_amount from train_with_new_features where blind_submodel = 'X.45.2';\"\n",
    "x_45_2 = pd.read_sql(sql, con).values\n",
    "def cal_woe(claims_in_lvl, all_claims):\n",
    "    \"\"\"\n",
    "        given a numpy array, return WOE\n",
    "    \"\"\"\n",
    "    freq_evnt =  sum(claims_in_lvl != 0) / sum(all_claims != 0)\n",
    "    freq_non_evnt =  sum(claims_in_lvl == 0) / sum(all_claims == 0)\n",
    "    return np.log(freq_evnt / freq_non_evnt) * 100\n",
    "woe_x_45 = cal_woe(x_45, all_claims)\n",
    "woe_x_45_2 = cal_woe(x_45_2, all_claims)\n",
    "print \"WOE\\nX_45: {0}\\nX_45_2: {1}\".format(float(woe_x_45),float(woe_x_45_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create a balanced training set using oversampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find out how many non-zero values\n",
    "sql = \"select count(row_id) from train_with_new_features where claim_amount <> 0;\"\n",
    "non_zero_count = str(int(pd.read_sql(sql, con).values))\n",
    "cursor.execute(\"Create index train_row_ind on train_with_new_features(row_id);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a temp table since \"Order by Random\" do not work in a union a query\n",
    "sql = \"create table train_zero_random as \" + \\\n",
    "       \"Select * from train_with_new_features \" + \\\n",
    "           \"where claim_amount = 0 \" + \\\n",
    "            \"Order By RANDOM() \" + \\\n",
    "            \"Limit \" + non_zero_count  + \";\"\n",
    "cursor.execute(sql)\n",
    "#crate a balanced train table\n",
    "sql = \"Create table train_balanced as \" + \\\n",
    "          \"Select * from train_zero_random \" + \\\n",
    "       \"Union \" + \\\n",
    "          \"select * from train_with_new_features \" + \\\n",
    "          \"where claim_amount <> 0\" \n",
    "cursor.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced sample count: 191210\n"
     ]
    }
   ],
   "source": [
    "print \"Balanced sample count: {0}\".format(int(non_zero_count) * 2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
